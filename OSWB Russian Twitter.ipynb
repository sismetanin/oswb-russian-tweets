{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv('/Users/sismetanin/Downloads/concated_twitter_v3.csv.gz', encoding='UTF-8', \n",
    "                 lineterminator='\\n', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['id_str'])\n",
    "df = df.dropna(subset=['user_name', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2815374/2815374 [00:01<00:00, 1804334.58it/s]\n"
     ]
    }
   ],
   "source": [
    "twitter_names = [user_name.lower().strip() for user_name in tqdm(df.drop_duplicates(subset=['user_id_str'])['user_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "clf_gender = load('VkGenderLogit.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17515729/17515729 [00:11<00:00, 1462871.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "chunks = [name.lower() for name in tqdm(df['user_name'])]\n",
    "chunks = list(set(chunks))\n",
    "chunks = np.array_split(chunks, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [02:50<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "name_to_gender = {}\n",
    "\n",
    "for chunk in tqdm(chunks):\n",
    "    genders = clf_gender.predict(chunk)\n",
    "    for i in range(len(genders)):\n",
    "        name_to_gender[chunk[i]]=genders[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17515729/17515729 [00:33<00:00, 518625.41it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_gender(name):\n",
    "    name = name.lower()\n",
    "    if name_to_gender[name]==1:\n",
    "        return 'F'\n",
    "    else:\n",
    "        return 'M'\n",
    "\n",
    "df['gender'] = [get_gender(name) for name in tqdm(df['user_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets 17515729\n",
      "Total users 2815374\n",
      "Tweets per user 6.221457255767795\n"
     ]
    }
   ],
   "source": [
    "print('Total tweets', len(df))\n",
    "print('Total users', len(df['user_id_str'].unique()))\n",
    "print('Tweets per user', len(df)/len(df['user_id_str'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets-ru-oswb-labeled.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping sentiment labels to interpretable classes\n",
    "mapper = {\n",
    "    'LABEL_0': 'negative', \n",
    "    'LABEL_1': 'neutral', \n",
    "    'LABEL_2': 'positive', \n",
    "    'LABEL_3': 'skip', \n",
    "    'LABEL_4': 'speech'\n",
    "}\n",
    "\n",
    "df['sentiment'] = [mapper[label] for label in df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['created_at'])\n",
    "dates = df['date'].dt.to_pydatetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17515729/17515729 [01:18<00:00, 221805.37it/s]\n"
     ]
    }
   ],
   "source": [
    "df['ymd'] = [date.strftime('%Y-%m-%d') for date in tqdm(dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17515729/17515729 [01:13<00:00, 239806.41it/s]\n"
     ]
    }
   ],
   "source": [
    "df['ym'] = [date.strftime('%Y-%m') for date in tqdm(dates)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_vciom = pd.read_excel('schastie.xls', index_col=0, skiprows=1)\n",
    "\n",
    "def get_vciom_months():\n",
    "    return sorted([date.strftime('%Y-%m') for date in df_vciom.iloc[5][1:].keys()])\n",
    "\n",
    "def get_vciom_happiness_index(months):\n",
    "    vciom_index = df_vciom.iloc[5][1:].values\n",
    "    vciom_index_months = [date.strftime('%Y-%m') for date in df_vciom.iloc[5][1:].keys()]\n",
    "    mapping = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(vciom_index_months)):\n",
    "        mapping[vciom_index_months[i]]=vciom_index[i]\n",
    "    return [mapping[month] for month in months]\n",
    "\n",
    "\n",
    "def get_vciom_positive_affect_index(months):\n",
    "    vciom_index = (df_vciom.iloc[0][1:].values+df_vciom.iloc[1][1:].values)/(df_vciom.iloc[0][1:].values\n",
    "                                                                             +df_vciom.iloc[1][1:].values\n",
    "                                                                             +df_vciom.iloc[2][1:].values\n",
    "                                                                             +df_vciom.iloc[3][1:].values\n",
    "                                                                             +df_vciom.iloc[4][1:].values)\n",
    "    \n",
    "    \n",
    "    vciom_index_months = [date.strftime('%Y-%m') for date in df_vciom.iloc[5][1:].keys()]\n",
    "    mapping = {}\n",
    "    for i in range(len(vciom_index_months)):\n",
    "        mapping[vciom_index_months[i]]=vciom_index[i]\n",
    "    return [mapping[month] for month in months]\n",
    "\n",
    "def get_vciom_negative_affect_index(months):\n",
    "    vciom_index = (df_vciom.iloc[2][1:].values+df_vciom.iloc[3][1:].values)/(df_vciom.iloc[0][1:].values\n",
    "                                                                             +df_vciom.iloc[1][1:].values\n",
    "                                                                             +df_vciom.iloc[2][1:].values\n",
    "                                                                             +df_vciom.iloc[3][1:].values\n",
    "                                                                             +df_vciom.iloc[4][1:].values)\n",
    "    \n",
    "    vciom_index_months = [date.strftime('%Y-%m') for date in df_vciom.iloc[5][1:].keys()]\n",
    "    mapping = {}\n",
    "    for i in range(len(vciom_index_months)):\n",
    "        mapping[vciom_index_months[i]]=vciom_index[i]\n",
    "    return [mapping[month] for month in months]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_months = [\n",
    "    '2014-04',\n",
    "    '2015-11',\n",
    "    '2016-04',\n",
    "    '2016-11',\n",
    "    '2017-07',\n",
    "    '2018-03',\n",
    "    '2018-07',\n",
    "    '2019-11',\n",
    "    '2020-04',\n",
    "    '2020-05',\n",
    "    '2020-06',\n",
    "    '2020-07',\n",
    "    '2020-08',\n",
    "    '2020-09',\n",
    "    '2020-10',\n",
    "    '2020-11',\n",
    "    '2020-12',\n",
    "    '2021-03',\n",
    "    '2021-04',\n",
    "    '2021-05'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "vciom_index = get_vciom_happiness_index(overlapping_months)\n",
    "vciom_pa_index = get_vciom_positive_affect_index(overlapping_months)\n",
    "vciom_na_index = get_vciom_negative_affect_index(overlapping_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before agg 1196653 after agg 377281\n",
      "before drop 1196653\n",
      "after sentiment drop 1100659\n",
      "after duplicates drop 343159\n",
      "before agg 831308 after agg 292331\n",
      "before drop 831308\n",
      "after sentiment drop 742831\n",
      "after duplicates drop 258791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:21<06:47, 21.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before agg 478242 after agg 156978\n",
      "before drop 478242\n",
      "after sentiment drop 439714\n",
      "after duplicates drop 141923\n",
      "before agg 293327 after agg 120183\n",
      "before drop 293327\n",
      "after sentiment drop 263212\n",
      "after duplicates drop 108120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:29<05:15, 17.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before agg 583026 after agg 171068\n",
      "before drop 583026\n",
      "after sentiment drop 538079\n",
      "after duplicates drop 154205\n",
      "before agg 328482 after agg 133062\n",
      "before drop 328482\n",
      "after sentiment drop 291658\n",
      "after duplicates drop 118554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:38<04:14, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before agg 514474 after agg 172084\n",
      "before drop 514474\n",
      "after sentiment drop 468050\n",
      "after duplicates drop 154482\n",
      "before agg 312215 after agg 122226\n",
      "before drop 312215\n",
      "after sentiment drop 277446\n",
      "after duplicates drop 108687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:47<03:29, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before agg 427100 after agg 127531\n",
      "before drop 427100\n",
      "after sentiment drop 392817\n",
      "after duplicates drop 114654\n",
      "before agg 173087 after agg 81553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:54<02:49, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 173087\n",
      "after sentiment drop 149403\n",
      "after duplicates drop 72386\n",
      "before agg 246532 after agg 104791\n",
      "before drop 246532\n",
      "after sentiment drop 218834\n",
      "after duplicates drop 94145\n",
      "before agg 127113 after agg 62707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [01:00<02:16,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 127113\n",
      "after sentiment drop 109777\n",
      "after duplicates drop 55909\n",
      "before agg 148616 after agg 73711\n",
      "before drop 148616\n",
      "after sentiment drop 128398\n",
      "after duplicates drop 65934\n",
      "before agg 84178 after agg 43350\n",
      "before drop 84178\n",
      "after sentiment drop 71925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [01:05<01:48,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after duplicates drop 38620\n",
      "before agg 148695 after agg 67769\n",
      "before drop 148695\n",
      "after sentiment drop 126537\n",
      "after duplicates drop 59389\n",
      "before agg 79997 after agg 37254\n",
      "before drop 79997\n",
      "after sentiment drop 67139\n",
      "after duplicates drop 32389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [01:10<01:27,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before agg 261239 after agg 94822\n",
      "before drop 261239\n",
      "after sentiment drop 226934\n",
      "after duplicates drop 82906\n",
      "before agg 145790 after agg 52410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [01:16<01:15,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 145790\n",
      "after sentiment drop 125958\n",
      "after duplicates drop 45406\n",
      "before agg 243310 after agg 90372\n",
      "before drop 243310\n",
      "after sentiment drop 210075\n",
      "after duplicates drop 78779\n",
      "before agg 138962 after agg 50304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [01:22<01:05,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 138962\n",
      "after sentiment drop 118892\n",
      "after duplicates drop 43347\n",
      "before agg 268467 after agg 94596\n",
      "before drop 268467\n",
      "after sentiment drop 231902\n",
      "after duplicates drop 82003\n",
      "before agg 152775 after agg 52234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [01:28<00:57,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 152775\n",
      "after sentiment drop 130765\n",
      "after duplicates drop 44815\n",
      "before agg 244178 after agg 89160\n",
      "before drop 244178\n",
      "after sentiment drop 209767\n",
      "after duplicates drop 77170\n",
      "before agg 143463 after agg 50119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [01:34<00:49,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 143463\n",
      "after sentiment drop 122622\n",
      "after duplicates drop 42944\n",
      "before agg 295541 after agg 100690\n",
      "before drop 295541\n",
      "after sentiment drop 254934\n",
      "after duplicates drop 86891\n",
      "before agg 175344 after agg 56638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [01:40<00:43,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 175344\n",
      "after sentiment drop 149865\n",
      "after duplicates drop 48330\n",
      "before agg 277026 after agg 97373\n",
      "before drop 277026\n",
      "after sentiment drop 237695\n",
      "after duplicates drop 83907\n",
      "before agg 166517 after agg 54716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [01:47<00:37,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 166517\n",
      "after sentiment drop 142154\n",
      "after duplicates drop 46611\n",
      "before agg 292623 after agg 100148\n",
      "before drop 292623\n",
      "after sentiment drop 252475\n",
      "after duplicates drop 86489\n",
      "before agg 176835 after agg 56859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [01:53<00:31,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 176835\n",
      "after sentiment drop 151255\n",
      "after duplicates drop 48484\n",
      "before agg 276318 after agg 95362\n",
      "before drop 276318\n",
      "after sentiment drop 238686\n",
      "after duplicates drop 82530\n",
      "before agg 168228 after agg 54173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [01:59<00:25,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 168228\n",
      "after sentiment drop 144189\n",
      "after duplicates drop 46248\n",
      "before agg 303179 after agg 99920\n",
      "before drop 303179\n",
      "after sentiment drop 263028\n",
      "after duplicates drop 86365\n",
      "before agg 183198 after agg 56361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [02:06<00:19,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 183198\n",
      "after sentiment drop 157610\n",
      "after duplicates drop 47946\n",
      "before agg 267055 after agg 93999\n",
      "before drop 267055\n",
      "after sentiment drop 229622\n",
      "after duplicates drop 81157\n",
      "before agg 163052 after agg 53430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [02:12<00:12,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 163052\n",
      "after sentiment drop 139196\n",
      "after duplicates drop 45303\n",
      "before agg 105658 after agg 52762\n",
      "before drop 105658\n",
      "after sentiment drop 88167\n",
      "after duplicates drop 46043\n",
      "before agg 63799 after agg 30507\n",
      "before drop 63799\n",
      "after sentiment drop 52041\n",
      "after duplicates drop 26179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [02:17<00:05,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before agg 237559 after agg 85837\n",
      "before drop 237559\n",
      "after sentiment drop 203367\n",
      "after duplicates drop 74094\n",
      "before agg 145842 after agg 48698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop 145842\n",
      "after sentiment drop 123122\n",
      "after duplicates drop 41103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from datetime import date\n",
    "from calendar import monthrange\n",
    "import numpy as np\n",
    "\n",
    "def aggregated_sentiment(x):\n",
    "    ordered = Counter(x).most_common()\n",
    "    if len(ordered) == 1:\n",
    "        return ordered[0][0]\n",
    "    elif ordered[0][1]>ordered[1][1]:\n",
    "        return ordered[0][0]\n",
    "    return np.nan\n",
    "\n",
    "    \n",
    "def aggregate_sentiment_per_user(temp_df, gender=None):\n",
    "    if gender is not None:\n",
    "        temp_df = temp_df[temp_df['gender']==gender]\n",
    "    temp_df = temp_df[temp_df['truncated']==False]\n",
    "    result = temp_df.groupby(['user_id_str'])['sentiment'].agg([aggregated_sentiment])\n",
    "    print('before agg', len(temp_df), 'after agg', len(result))\n",
    "    user_to_sentiment = {}\n",
    "    for i in range(len(result)):\n",
    "        user_to_sentiment[result.index[i]] = result['aggregated_sentiment'].values[i]         \n",
    "    temp_df['sentiment'] = [user_to_sentiment[userId] for userId in temp_df['user_id_str'].values]\n",
    "    print('before drop', len(temp_df))\n",
    "    temp_df = temp_df.dropna(subset=['sentiment'])\n",
    "    print('after sentiment drop', len(temp_df))\n",
    "    temp_df = temp_df.drop_duplicates(subset=['user_id_str'])\n",
    "    print('after duplicates drop', len(temp_df))\n",
    "    return temp_df\n",
    "\n",
    "positive_count_m = []\n",
    "negative_count_m = []\n",
    "neutral_count_m = []\n",
    "overall_count_m = []\n",
    "speech_count_m = []\n",
    "skip_count_m = []\n",
    "\n",
    "positive_count_f = []\n",
    "negative_count_f = []\n",
    "neutral_count_f = []\n",
    "overall_count_f = []\n",
    "speech_count_f = []\n",
    "skip_count_f = []\n",
    "\n",
    "tweets_total = []\n",
    "users = []\n",
    "for ymd in tqdm(overlapping_months):\n",
    "    temp = df[df['ym']==ymd]\n",
    "    tweets_total.append(len(temp))\n",
    "    users.append(df['user_id_str'].unique())\n",
    "    temp = aggregate_sentiment_per_user(temp, gender='M')\n",
    "    positive_count_m.append(len(temp[temp['sentiment']=='positive']))\n",
    "    negative_count_m.append(len(temp[temp['sentiment']=='negative']))\n",
    "    neutral_count_m.append(len(temp[temp['sentiment']=='neutral']))\n",
    "    speech_count_m.append(len(temp[temp['sentiment']=='speech']))\n",
    "    skip_count_m.append(len(temp[temp['sentiment']=='skip']))\n",
    "    overall_count_m.append(len(temp))\n",
    "    \n",
    "    temp = df[df['ym']==ymd]\n",
    "    temp = aggregate_sentiment_per_user(temp, gender='F')\n",
    "    positive_count_f.append(len(temp[temp['sentiment']=='positive']))\n",
    "    negative_count_f.append(len(temp[temp['sentiment']=='negative']))\n",
    "    neutral_count_f.append(len(temp[temp['sentiment']=='neutral']))\n",
    "    speech_count_f.append(len(temp[temp['sentiment']=='speech']))\n",
    "    skip_count_f.append(len(temp[temp['sentiment']=='skip']))\n",
    "    overall_count_f.append(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:32<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "positive_count = []\n",
    "negative_count = []\n",
    "neutral_count = []\n",
    "overall_count = []\n",
    "speech_count = []\n",
    "skip_count = []\n",
    "users_count = []\n",
    "user_ids = []\n",
    "\n",
    "for ymd in tqdm(overlapping_months):\n",
    "    temp = df[df['ym']==ymd]\n",
    "    temp = temp[temp['truncated']==False]\n",
    "    temp = temp.dropna(subset=['sentiment'])\n",
    "    positive_count.append(len(temp[temp['sentiment']=='positive']))\n",
    "    negative_count.append(len(temp[temp['sentiment']=='negative']))\n",
    "    neutral_count.append(len(temp[temp['sentiment']=='neutral']))\n",
    "    speech_count.append(len(temp[temp['sentiment']=='speech']))\n",
    "    skip_count.append(len(temp[temp['sentiment']=='skip']))\n",
    "    overall_count.append(len(temp))\n",
    "    user_ids.extend(temp['user_id_str'].unique())\n",
    "    users_count.append(temp['user_id_str'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets 10869003\n",
      "Unique users 1955827\n",
      "Tweets per user 5.557241514714748\n"
     ]
    }
   ],
   "source": [
    "print('Total tweets', sum(overall_count))\n",
    "print('Unique users', len(set(user_ids)))\n",
    "print('Tweets per user', sum(overall_count)/len(set(user_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_count = np.array(positive_count)\n",
    "negative_count = np.array(negative_count)\n",
    "neutral_count = np.array(neutral_count)\n",
    "overall_count = np.array(overall_count)\n",
    "speech_acount = np.array(speech_count)\n",
    "skip_count = np.array(skip_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_count_m = np.array(positive_count_m)\n",
    "negative_count_m = np.array(negative_count_m)\n",
    "neutral_count_m = np.array(neutral_count_m)\n",
    "overall_count_m = np.array(overall_count_m)\n",
    "speech_count_m = np.array(speech_count_m)\n",
    "skip_count_m = np.array(skip_count_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_count_f = np.array(positive_count_f)\n",
    "negative_count_f = np.array(negative_count_f)\n",
    "neutral_count_f = np.array(neutral_count_f)\n",
    "overall_count_f = np.array(overall_count_f)\n",
    "speech_count_f = np.array(speech_count_f)\n",
    "skip_count_f = np.array(skip_count_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "20\n",
      "                  VCIOM Net   VCIOM PA    VCIOM NA\n",
      "P-N/All plain        0.0854     0.1809      -0.115\n",
      "P-N/All w           -0.1109     0.0321      0.1841\n",
      "P-N/Neu plain        0.0458     0.1622     -0.0602\n",
      "P-N/Neu w           -0.0933     0.0624      0.1664\n",
      "P/All plain          0.0167    -0.0241     -0.1027\n",
      "P/All w             0.469**   0.5177**     -0.2602\n",
      "P/Neutral plain      0.0458    -0.0062     -0.1168\n",
      "P/Neutral w        0.5332**    0.548**     -0.3292\n",
      "N/Neutral plain     -0.1117    -0.1372       0.092\n",
      "N/Neutral w           0.366     0.3012     -0.2496\n",
      "N/All plain         -0.0211    -0.1497      0.0053\n",
      "N/All w              0.3476     0.2602      -0.262\n",
      "VCIOM Net            1.0***  0.7945***  -0.9055***\n",
      "VCIOM PA          0.7945***     1.0***    -0.569**\n",
      "VCIOM NA         -0.9055***   -0.569**      1.0***\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "{} &   VCIOM Net &   VCIOM PA &    VCIOM NA \\\\\n",
      "\\midrule\n",
      "P-N/All plain   &      0.0854 &     0.1809 &      -0.115 \\\\\n",
      "P-N/All w       &     -0.1109 &     0.0321 &      0.1841 \\\\\n",
      "P-N/Neu plain   &      0.0458 &     0.1622 &     -0.0602 \\\\\n",
      "P-N/Neu w       &     -0.0933 &     0.0624 &      0.1664 \\\\\n",
      "P/All plain     &      0.0167 &    -0.0241 &     -0.1027 \\\\\n",
      "P/All w         &     0.469** &   0.5177** &     -0.2602 \\\\\n",
      "P/Neutral plain &      0.0458 &    -0.0062 &     -0.1168 \\\\\n",
      "P/Neutral w     &    0.5332** &    0.548** &     -0.3292 \\\\\n",
      "N/Neutral plain &     -0.1117 &    -0.1372 &       0.092 \\\\\n",
      "N/Neutral w     &       0.366 &     0.3012 &     -0.2496 \\\\\n",
      "N/All plain     &     -0.0211 &    -0.1497 &      0.0053 \\\\\n",
      "N/All w         &      0.3476 &     0.2602 &      -0.262 \\\\\n",
      "VCIOM Net       &      1.0*** &  0.7945*** &  -0.9055*** \\\\\n",
      "VCIOM PA        &   0.7945*** &     1.0*** &    -0.569** \\\\\n",
      "VCIOM NA        &  -0.9055*** &   -0.569** &      1.0*** \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.stats.stats import pearsonr, spearmanr    \n",
    "\n",
    "m = {\n",
    "\n",
    "    'P-N/All plain': (positive_count-negative_count)/(overall_count),\n",
    "    'P-N/All w': (positive_count_m-negative_count_m)/overall_count_m*0.47+(positive_count_f-negative_count_f)/overall_count_f*0.53,\n",
    "    \n",
    "    'P-N/Neu plain':  (positive_count-negative_count)/(neutral_count),\n",
    "    'P-N/Neu w': (positive_count_m-negative_count_m)/(neutral_count_m)*0.47+(positive_count_f-negative_count_f)/(neutral_count_f)*0.53,\n",
    "     \n",
    "    'P/All plain':  (positive_count)/(overall_count),\n",
    "    'P/All w': positive_count_m/overall_count_m*0.47+positive_count_f/overall_count_f*0.53,\n",
    "    \n",
    "    'P/Neutral plain':  (positive_count)/(neutral_count),\n",
    "    'P/Neutral w': positive_count_m/neutral_count_m*0.47+positive_count_f/neutral_count_f*0.53,\n",
    "     \n",
    "    'N/Neutral plain':  (negative_count)/(neutral_count),\n",
    "    'N/Neutral w': negative_count_m/neutral_count_m*0.47+negative_count_f/neutral_count_f*0.53,\n",
    "    \n",
    "    'N/All plain':  (negative_count)/(overall_count),\n",
    "    'N/All w': negative_count_m/overall_count_m*0.47+negative_count_f/overall_count_f*0.53,\n",
    "    \n",
    "    'VCIOM Net': np.array(vciom_index)/100,\n",
    "    'VCIOM PA': np.array(vciom_pa_index),\n",
    "    'VCIOM NA': np.array(vciom_na_index)\n",
    "}\n",
    "\n",
    "def get_corr_func(method):\n",
    "    if  method=='spearman':\n",
    "        return spearmanr\n",
    "    elif method=='pearson':\n",
    "        return pearsonr\n",
    "    elif method=='kendall':\n",
    "        return stats.kendalltau\n",
    "    \n",
    "def get_corr(m, method='spearman'):\n",
    "    print(i)\n",
    "    df_corr = pd.DataFrame(m)\n",
    "    df_corr = pd.DataFrame({c: df_corr[c].astype(float).diff() for c in df_corr.columns}) #[2:-2]\n",
    "    print(len(df_corr))\n",
    "    rho = df_corr.corr(method=method)\n",
    "    pval = df_corr.corr(method=lambda x, y: get_corr_func(method)(x, y)[1]) - np.eye(*rho.shape)\n",
    "    p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\n",
    "    return rho.round(4).astype(str) + p\n",
    "t = get_corr(m, 'spearman')[['VCIOM Net', 'VCIOM PA', 'VCIOM NA']]\n",
    "print(t)\n",
    "print(t.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
